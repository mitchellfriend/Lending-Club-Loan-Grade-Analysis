{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import  ExtraTreesClassifier\n",
    "\n",
    "#ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#set show all columns\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "#plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df_reference = pd.read_csv('data_YearsAdded_OutliersDropped.csv', sep=',')\n",
    "\n",
    "df=df_reference\n",
    "\n",
    "#create binary response variable\n",
    "df[\"depAB\"] = np.where(np.isin(df['grade'], [\"A\", \"B\"]), 1 , 0)\n",
    "\n",
    "#make bins for employee length\n",
    "df[\"emp_length_new\"] = np.where(df['emp_length']==\"10+ years\", \"10+ years\",\n",
    "                                     np.where(np.isin(df['emp_length'], [\"1 year\",'2 years','3 years','4 years','5 years']), \"1-5 years\",\n",
    "                                              np.where(np.isin(df['emp_length'], ['6 years','7 years','8 years','9 years']), \"6-9 years\",\n",
    "                                                       np.where(np.isin(df['emp_length'], [\"< 1 year\"]), \"<1 year\", \"n/a\"))))\n",
    "df = df.drop('emp_length', axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = df\n",
    "pre_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection\n",
    "\n",
    "pre_df = pre_df.loc[df_reference['Year'] == 2014]\n",
    "\n",
    "\n",
    "y=['depAB']\n",
    "X=['acc_open_past_24mths_x', 'annual_inc_x', 'bc_util_x',\n",
    "       'dti_x', 'inq_last_6mths_x', 'mo_sin_old_rev_tl_op_x',\n",
    "       'mo_sin_rcnt_rev_tl_op_x', 'mo_sin_rcnt_tl_x', 'mort_acc_x',\n",
    "       'mths_since_recent_bc_x', 'num_accts_ever_120_pd_x', 'num_actv_bc_tl_x',\n",
    "       'num_il_tl_x', 'num_bc_sats_x', 'open_acc_x', 'pub_rec_bankruptcies_x',\n",
    "       'pub_rec_x', 'recoveries_x', 'tot_cur_bal_x', 'total_bc_limit_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(pre_df[X], pre_df[y], random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run model\n",
    "#Grid Search\n",
    "rnd_clf = RandomForestClassifier(random_state=42, \n",
    "                                 n_jobs=-1,\n",
    "                                 max_depth=8,\n",
    "                                 n_estimators=500,\n",
    "                                 max_features=5, \n",
    "                                 criterion='gini')\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random search\n",
    "rnd_clf = RandomForestClassifier(bootstrap=True,\n",
    "                                 max_depth=20,\n",
    "                                 max_features='sqrt',\n",
    "                                 min_samples_leaf=2,\n",
    "                                 min_samples_split=2,\n",
    "                                 n_estimators=1200)\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(KNeighborsClassifier(n_jobs=-1),\n",
    "                             max_samples=0.5, max_features=5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bagging_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(pre_df[X], pre_df[y])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "log_clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                          multi_class='multinomial').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score\n",
    "y_pred = log_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini',\n",
    "                                                        max_depth=2,\n",
    "                                                        max_leaf_nodes=5,\n",
    "                                                        min_samples_leaf=10,\n",
    "                                                        min_samples_split=2),\n",
    "                                                       learning_rate=0.1,\n",
    "                                                       n_estimators=500,\n",
    "                                                       random_state=29)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "     max_depth=3, random_state=0).fit(X_train, y_train)\n",
    "GB_clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Naieve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_clf = GaussianNB()\n",
    "NB_clf.fit(pre_df[X], pre_df[y])\n",
    "\n",
    "\n",
    "NB_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = NB_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "XGB_clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.8, gamma=0.1, learning_rate=0.0005,\n",
    "       max_delta_step=0, max_depth=8, min_child_weight=15, missing=None,\n",
    "       n_estimators=10000, n_jobs=-1, nthread=None,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
    "       subsample=0.5)\n",
    "XGB_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = XGB_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = \"center\"> Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[ ('rf', rnd_clf),                              \n",
    "               ('Ada', ada_clf),\n",
    "               ('GB', GB_clf),               \n",
    "               ('XGB', XGB_clf)],\n",
    "    n_jobs=-1,\n",
    "    voting='hard') #Predict the class with the highest class probability averaged over all individual classifiers\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "for clf in ( rnd_clf,\n",
    "            \n",
    "            ada_clf,\n",
    "            GB_clf,\n",
    "            \n",
    "            XGB_clf,\n",
    "            voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record the time it takes to complete grid search\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define grid search\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "    'min_samples_split' :[2,3,4,5],\n",
    "    \n",
    "    'bootstrap' :[True, False]\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(x_train, y_train)\n",
    "\n",
    "#print parameters\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10, 15, 20],\n",
    "        'gamma': [0.1, 0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.4, 0.5, 0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.2, 0.3, 0.4, 0.5,0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'n_estimators': [200, 500, 1000]\n",
    "    \n",
    "        }\n",
    "xgb = XGBClassifier(learning_rate=0.005, objective='binary:logistic',\n",
    "                    silent=True, n_jobs=-1)\n",
    "\n",
    "folds = 5\n",
    "param_comb = 50\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb,\n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=param_comb,\n",
    "                                   scoring='roc_auc',\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=skf.split(X_train,y_train),\n",
    "                                   verbose=3,\n",
    "                                   random_state=42 )\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search.fit(X_train, y_train)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"n_estimators\": [100, 500, 1000],\n",
    "              \"max_depth\" : [1, 2, 3, 4, 5, 6, 7],\n",
    "              \"learning_rate\" : [0.5, 0.25, 0.05]\n",
    "              \n",
    "             }\n",
    "\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state = 42,\n",
    "                             max_features = \"auto\",\n",
    "                             class_weight = \"auto\",\n",
    "                             max_depth = None)\n",
    "\n",
    "ABC = AdaBoostClassifier(base_estimator = DTC)\n",
    "\n",
    "# run grid search\n",
    "grid_search_ABC = GridSearchCV(ABC, param_grid=param_grid, scoring = 'roc_auc')\n",
    "grid_search_ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def tune_score_model(model, param_grid, X, y, n_jobs=-1, cv=3):\n",
    "    \n",
    "    #Runs a GridSearchCV for the model and param_grid passed into the function\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', n_jobs=n_jobs, verbose=1, cv=cv)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    #Returns the best score and params used to get the score\n",
    "    return grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Graphing Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important features Graph\n",
    "\n",
    "import seaborn as sns\n",
    "classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
    "                                                                      max_depth=1,\n",
    "                                                                      max_leaf_nodes=None,\n",
    "                                                                     min_samples_leaf=5,\n",
    "                                                                      min_samples_split=2), \n",
    "                                                                    learning_rate=0.1,\n",
    "                                                                    n_estimators=500,\n",
    "                                                                    random_state=29)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "feature_imp = pd.DataFrame(data=classifier.feature_importances_, index=X_train.columns.values, columns=['values'])\n",
    "feature_imp.sort_values(['values'], ascending=False, inplace=True)\n",
    "feature_imp.reset_index(level=0, inplace=True)\n",
    "sns.barplot(x='index', y='values', data=feature_imp, palette='deep')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import seaborn as sns\n",
    "classifier = rnd_clf = RandomForestClassifier(bootstrap=True,\n",
    "                                 max_depth=20,\n",
    "                                 max_features='sqrt',\n",
    "                                 min_samples_leaf=2,\n",
    "                                 min_samples_split=2,\n",
    "                                 n_estimators=1200)#important features Graph\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "feature_imp = pd.DataFrame(data=classifier.feature_importances_, index=X_train.columns.values, columns=['values'])\n",
    "feature_imp.sort_values(['values'], ascending=False, inplace=True)\n",
    "feature_imp.reset_index(level=0, inplace=True)\n",
    "sns.barplot(x='index', y='values', data=feature_imp, palette='deep')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"> Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import scipy as sp\n",
    "\n",
    "def tune(X , y, search_type, n_iter):\n",
    "    scores = []\n",
    "    params = []\n",
    "    for i in range(len(n_iter)):\n",
    "        scaler = StandardScaler()\n",
    "        clf = SVC()\n",
    "        pipe = Pipeline(steps=[('scaler', scaler), \n",
    "                               ('svc', clf)])\n",
    "        if search_type == 'grid':\n",
    "            param_grid = dict(svc__C = np.logspace(-2, 5, np.round(n_iter[i]**0.5)), svc__gamma = np.logspace(-5, 1, np.round(n_iter[i]**0.5)))\n",
    "            gridsearch = GridSearchCV(pipe, param_grid = param_grid, cv = 3)\n",
    "            gridsearch.fit(X, y)\n",
    "            scores.append(gridsearch.best_score_)\n",
    "            params.append(gridsearch.best_params_)\n",
    "        elif search_type == 'random':\n",
    "            param_distributions = {'svc__C': sp.stats.expon(scale=10), \n",
    "            'svc__gamma': sp.stats.expon(scale=0.1)}\n",
    "            randsearch = RandomizedSearchCV(pipe, param_distributions = param_distributions, n_iter= n_iter[i], cv = 3, random_state = 333)\n",
    "            randsearch.fit(X, y)\n",
    "            scores.append(randsearch.best_score_)\n",
    "            params.append(randsearch.best_params_)\n",
    "        \n",
    "        print(search_type, \"with\", str(n_iter[i]), \"iterations completed\")\n",
    "    \n",
    "    return scores, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = [9, 25, 64, 100, 169]\n",
    "\n",
    "\n",
    "scores_random, params_random = tune(X_train, y_train, 'random', n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Post Loan Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = df.loc[df_reference['Year'] != 2014]\n",
    "\n",
    "\n",
    "y=['depAB']\n",
    "X=['acc_open_past_24mths_x', 'annual_inc_x', 'bc_util_x',\n",
    "       'dti_x', 'inq_last_6mths_x', 'mo_sin_old_rev_tl_op_x',\n",
    "       'mo_sin_rcnt_rev_tl_op_x', 'mo_sin_rcnt_tl_x', 'mort_acc_x',\n",
    "       'mths_since_recent_bc_x', 'num_accts_ever_120_pd_x', 'num_actv_bc_tl_x',\n",
    "       'num_il_tl_x', 'num_bc_sats_x', 'open_acc_x', 'pub_rec_bankruptcies_x',\n",
    "       'pub_rec_x', 'recoveries_x', 'tot_cur_bal_x', 'total_bc_limit_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(post_df[X], post_df[y], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "XGB_clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.8, gamma=0.1, learning_rate=0.0005,\n",
    "       max_delta_step=0, max_depth=8, min_child_weight=15, missing=None,\n",
    "       n_estimators=10000, n_jobs=-1, nthread=None,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
    "       subsample=0.5)\n",
    "XGB_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = XGB_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2014 model on 2016/17 data\n",
    "#Train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(pre_df[X], pre_df[y], random_state=42)\n",
    "XGB_clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.8, gamma=0.1, learning_rate=0.0005,\n",
    "       max_delta_step=0, max_depth=8, min_child_weight=15, missing=None,\n",
    "       n_estimators=10000, n_jobs=-1, nthread=None,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
    "       subsample=0.5)\n",
    "XGB_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = XGB_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(post_df[X], post_df[y], random_state=42)\n",
    "\n",
    "\n",
    "y_pred = XGB_clf.predict(X_test)\n",
    "print( accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
